{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dateutil.parser import parse\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set =  'feature_set_4_normalised'\n",
    "X_train = pd.read_csv( F'./data/{feature_set}/X_train_full.csv').drop(columns=['ID_JOIN'])\n",
    "y_train = pd.read_csv( F'./data/{feature_set}/y_train.csv').values.ravel()\n",
    "\n",
    "X_val = pd.read_csv( F'./data/{feature_set}/X_valid_full.csv').drop(columns=['ID_JOIN'])\n",
    "y_val = pd.read_csv( F'./data/{feature_set}/y_valid.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Sampling: Counter({0.0: 727178, 1.0: 19329})\n",
      "Weight for class 0: 0.51\n",
      "Weight for class 1: 19.31\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_train)\n",
    "print(\"Before Sampling: {}\".format(counter))\n",
    "\n",
    "pos = Counter(y_train).get(1)\n",
    "neg = Counter(y_train).get(0)\n",
    "total = neg+pos\n",
    "\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_LR = LogisticRegression(max_iter=10000,random_state=42,verbose=1,class_weight=class_weight)\n",
    "# model_LR.fit(X_train,y_train)\n",
    "\n",
    "# val_res_LR = 1 -model_LR.predict_proba(X_val)[:,0]\n",
    "# roc_auc_score(y_val,val_res_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.656 GB of training data: 2.661 s\n",
      "Binning 0.073 GB of validation data: 0.113 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/1000] 1 tree, 8107 leaves, max depth = 48, train loss: 0.08687, val loss: 0.09607, in 7.611s\n",
      "[2/1000] 1 tree, 22755 leaves, max depth = 59, train loss: 0.07834, val loss: 0.09477, in 19.226s\n",
      "[3/1000] 1 tree, 26252 leaves, max depth = 56, train loss: 0.07103, val loss: 0.09400, in 20.395s\n",
      "[4/1000] 1 tree, 26192 leaves, max depth = 60, train loss: 0.06456, val loss: 0.09338, in 20.108s\n",
      "[5/1000] 1 tree, 26233 leaves, max depth = 61, train loss: 0.05876, val loss: 0.09278, in 19.774s\n",
      "[6/1000] 1 tree, 26183 leaves, max depth = 58, train loss: 0.05362, val loss: 0.09225, in 19.911s\n",
      "[7/1000] 1 tree, 26154 leaves, max depth = 59, train loss: 0.04902, val loss: 0.09175, in 19.726s\n",
      "[8/1000] 1 tree, 26152 leaves, max depth = 65, train loss: 0.04492, val loss: 0.09137, in 20.313s\n",
      "[9/1000] 1 tree, 26112 leaves, max depth = 54, train loss: 0.04119, val loss: 0.09113, in 19.904s\n",
      "[10/1000] 1 tree, 26189 leaves, max depth = 58, train loss: 0.03783, val loss: 0.09086, in 19.658s\n",
      "[11/1000] 1 tree, 26053 leaves, max depth = 58, train loss: 0.03478, val loss: 0.09058, in 20.074s\n",
      "[12/1000] 1 tree, 26094 leaves, max depth = 58, train loss: 0.03203, val loss: 0.09046, in 20.199s\n",
      "[13/1000] 1 tree, 26223 leaves, max depth = 57, train loss: 0.02952, val loss: 0.09048, in 20.531s\n",
      "[14/1000] 1 tree, 26194 leaves, max depth = 61, train loss: 0.02723, val loss: 0.09054, in 19.894s\n",
      "[15/1000] 1 tree, 26128 leaves, max depth = 53, train loss: 0.02512, val loss: 0.09054, in 20.412s\n",
      "[16/1000] 1 tree, 26204 leaves, max depth = 63, train loss: 0.02322, val loss: 0.09062, in 19.797s\n",
      "[17/1000] 1 tree, 26179 leaves, max depth = 61, train loss: 0.02146, val loss: 0.09084, in 19.919s\n",
      "[18/1000] 1 tree, 26212 leaves, max depth = 62, train loss: 0.01986, val loss: 0.09115, in 19.668s\n",
      "[19/1000] 1 tree, 26152 leaves, max depth = 61, train loss: 0.01839, val loss: 0.09149, in 20.548s\n",
      "[20/1000] 1 tree, 26228 leaves, max depth = 75, train loss: 0.01705, val loss: 0.09197, in 19.917s\n",
      "[21/1000] 1 tree, 26259 leaves, max depth = 62, train loss: 0.01582, val loss: 0.09242, in 20.234s\n",
      "[22/1000] 1 tree, 26183 leaves, max depth = 84, train loss: 0.01469, val loss: 0.09297, in 19.772s\n",
      "Fit 22 trees in 432.619 s, (554438 total leaves)\n",
      "Time spent computing histograms: 316.337s\n",
      "Time spent finding best splits:  24.916s\n",
      "Time spent applying splits:      34.297s\n",
      "Time spent predicting:           0.452s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7617009913604174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_HGB = HistGradientBoostingClassifier(max_iter=1000,random_state=42,verbose=1,max_leaf_nodes=None,early_stopping=True,validation_fraction=0.1,n_iter_no_change=10)\n",
    "model_HGB.fit(X_train,y_train)\n",
    "val_res_HGB = 1 -model_HGB.predict_proba(X_val)[:,0]\n",
    "roc_auc_score(y_val,val_res_HGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ABC = AdaBoostClassifier(random_state=42,n_estimators=50)\n",
    "# model_ABC.fit(X_train,y_train)\n",
    "# val_res_ABC = 1 -model_ABC.predict_proba(X_val)[:,0]\n",
    "# roc_auc_score(y_val,val_res_ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed: 16.3min remaining: 48.8min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed: 17.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    8.2s remaining:   24.8s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7798406990610358"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BC = BaggingClassifier(random_state=42,n_estimators=100,n_jobs=-1,verbose=1)\n",
    "model_BC.fit(X_train,y_train)\n",
    "val_res_ABC = 1 -model_BC.predict_proba(X_val)[:,0]\n",
    "roc_auc_score(y_val,val_res_ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv( F'./data/{feature_set}/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'TX_ID':['ddaa070acea087eae360225e92c1609cea905e43'],\n",
    "        'TX_FRAUD':[0]\n",
    "       }\n",
    "df2 = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_HGB = 1 -model_HGB.predict_proba(test_df.drop(columns=['TX_ID','ID_JOIN']))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['TX_FRAUD']  = test_res_HGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_HGB = test_df[['TX_ID','TX_FRAUD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_HGB = pd.concat([submission_HGB,df2],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trans = pd.read_csv( F'./data/transactions_test.csv')\n",
    "test_trans = test_trans['TX_ID']\n",
    "submission_HGB.set_index('TX_ID',inplace=True)\n",
    "submission = submission_HGB.reindex(index =test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'results'\n",
    "submission.to_csv(F'./{location}/results_4_HGB.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_BC = 1 -model_HGB.predict_proba(test_df.drop(columns=['TX_ID','ID_JOIN','TX_FRAUD']))[:,0]\n",
    "test_df['TX_FRAUD']  = test_res_BC\n",
    "submission_BC = test_df[['TX_ID','TX_FRAUD']]\n",
    "submission_BC = pd.concat([submission_BC,df2],ignore_index = True)\n",
    "\n",
    "submission_BC.set_index('TX_ID',inplace=True)\n",
    "submission_BC = submission_BC.reindex(index =test_trans)\n",
    "\n",
    "location = 'results'\n",
    "submission.to_csv(F'./{location}/results_4_BC.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
